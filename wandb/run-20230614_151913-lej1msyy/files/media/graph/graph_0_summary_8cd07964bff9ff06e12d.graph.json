{"format": "torch", "nodes": [{"name": "ganlayer", "id": 1618922848064, "class_name": "GANLayer(\n  (gat_layers): ModuleList(\n    (0): GraphTransformerLayer(in_channels=128, out_channels=128, heads=1, residual=True)\n  )\n  (embedding_lap_pos_enc): Linear(in_features=128, out_features=128, bias=True)\n  (embedding_h): Linear(in_features=1147, out_features=128, bias=True)\n)", "parameters": [["gat_layers.0.attention.Q.weight", [128, 128]], ["gat_layers.0.attention.K.weight", [128, 128]], ["gat_layers.0.attention.V.weight", [128, 128]], ["gat_layers.0.O.weight", [128, 128]], ["gat_layers.0.O.bias", [128]], ["gat_layers.0.batch_norm1.weight", [128]], ["gat_layers.0.batch_norm1.bias", [128]], ["gat_layers.0.FFN_layer1.weight", [256, 128]], ["gat_layers.0.FFN_layer1.bias", [256]], ["gat_layers.0.FFN_layer2.weight", [128, 256]], ["gat_layers.0.FFN_layer2.bias", [128]], ["gat_layers.0.batch_norm2.weight", [128]], ["gat_layers.0.batch_norm2.bias", [128]], ["embedding_lap_pos_enc.weight", [128, 128]], ["embedding_lap_pos_enc.bias", [128]], ["embedding_h.weight", [128, 1147]], ["embedding_h.bias", [128]]], "output_shape": [[1147, 128]], "num_parameters": [16384, 16384, 16384, 16384, 128, 128, 128, 32768, 256, 32768, 128, 128, 128, 16384, 128, 146816, 128]}, {"name": "mlplayer", "id": 1618922847440, "class_name": "MLPLayer(\n  (mlp_layer1): Linear(in_features=256, out_features=512, bias=True)\n  (mlp_layer2): Linear(in_features=512, out_features=256, bias=True)\n  (mlp_layer3): Linear(in_features=256, out_features=1, bias=True)\n)", "parameters": [["mlp_layer1.weight", [512, 256]], ["mlp_layer1.bias", [512]], ["mlp_layer2.weight", [256, 512]], ["mlp_layer2.bias", [256]], ["mlp_layer3.weight", [1, 256]], ["mlp_layer3.bias", [1]]], "output_shape": [[4316]], "num_parameters": [131072, 512, 131072, 256, 256, 1]}], "edges": []}