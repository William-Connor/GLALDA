{"format": "torch", "nodes": [{"name": "ganlayer", "id": 1958283393536, "class_name": "GANLayer(\n  (gat_layers): ModuleList(\n    (0): GraphTransformerLayer(in_channels=256, out_channels=256, heads=1, residual=True)\n  )\n  (embedding_lap_pos_enc): Linear(in_features=256, out_features=256, bias=True)\n  (embedding_h): Linear(in_features=1147, out_features=256, bias=True)\n)", "parameters": [["gat_layers.0.attention.Q.weight", [256, 256]], ["gat_layers.0.attention.K.weight", [256, 256]], ["gat_layers.0.attention.V.weight", [256, 256]], ["gat_layers.0.O.weight", [256, 256]], ["gat_layers.0.O.bias", [256]], ["gat_layers.0.batch_norm1.weight", [256]], ["gat_layers.0.batch_norm1.bias", [256]], ["gat_layers.0.FFN_layer1.weight", [512, 256]], ["gat_layers.0.FFN_layer1.bias", [512]], ["gat_layers.0.FFN_layer2.weight", [256, 512]], ["gat_layers.0.FFN_layer2.bias", [256]], ["gat_layers.0.batch_norm2.weight", [256]], ["gat_layers.0.batch_norm2.bias", [256]], ["embedding_lap_pos_enc.weight", [256, 256]], ["embedding_lap_pos_enc.bias", [256]], ["embedding_h.weight", [256, 1147]], ["embedding_h.bias", [256]]], "output_shape": [[1147, 256]], "num_parameters": [65536, 65536, 65536, 65536, 256, 256, 256, 131072, 512, 131072, 256, 256, 256, 65536, 256, 293632, 256]}, {"name": "mlplayer", "id": 1958283393392, "class_name": "MLPLayer(\n  (mlp_layer1): Linear(in_features=512, out_features=128, bias=True)\n  (mlp_layer2): Linear(in_features=128, out_features=512, bias=True)\n  (mlp_layer3): Linear(in_features=512, out_features=1, bias=True)\n)", "parameters": [["mlp_layer1.weight", [128, 512]], ["mlp_layer1.bias", [128]], ["mlp_layer2.weight", [512, 128]], ["mlp_layer2.bias", [512]], ["mlp_layer3.weight", [1, 512]], ["mlp_layer3.bias", [1]]], "output_shape": [[4314]], "num_parameters": [65536, 128, 65536, 512, 512, 1]}], "edges": []}