{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score, precision_score, recall_score, f1_score, roc_curve, precision_recall_curve, auc\n",
    "from sklearn import preprocessing\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDAformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class valueEmbedding(nn.Module):\n",
    "    def __init__(self, d_input, d_model, value_linear, value_sqrt):\n",
    "        super(valueEmbedding, self).__init__()\n",
    "\n",
    "        self.value_linear = value_linear\n",
    "        self.value_sqrt = value_sqrt\n",
    "        self.d_model = d_model\n",
    "        self.inputLinear = nn.Linear(d_input, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.value_linear:\n",
    "            x = self.inputLinear(x)\n",
    "        if self.value_sqrt:\n",
    "            x = x * math.sqrt(self.d_model)\n",
    "        return x\n",
    "\n",
    "class positionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(positionalEmbedding, self).__init__()\n",
    "        \n",
    "        pos_emb = torch.zeros(max_len, d_model).float()\n",
    "        pos_emb.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        pos_emb[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_emb[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pos_emb = pos_emb.unsqueeze(0)\n",
    "        self.register_buffer('pos_emb', pos_emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pos_emb[:, :x.size(1)]\n",
    "\n",
    "class dataEmbedding(nn.Module):\n",
    "    def __init__(self, d_input, d_model, value_linear, value_sqrt, posi_emb=False, input_dropout=0.05):\n",
    "        super(dataEmbedding, self).__init__()\n",
    "        \n",
    "        self.posi_emb = posi_emb\n",
    "        self.value_embedding = valueEmbedding(d_input=d_input, d_model=d_model, value_linear=value_linear, value_sqrt=value_sqrt)\n",
    "        self.positional_embedding = positionalEmbedding(d_model=d_model)\n",
    "        self.inputDropout = nn.Dropout(input_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.posi_emb:\n",
    "            x = self.value_embedding(x) + self.positional_embedding(x)\n",
    "        else:\n",
    "            x = self.value_embedding(x)\n",
    "        return self.inputDropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Dot-Product Attention\n",
    "$$\\Large{Attention(Q, K, V) = softmax_k(\\frac{QK^T}{\\sqrt{d_k}}) V} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class scaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(scaledDotProductAttention, self).__init__()\n",
    "        \n",
    "    def forward(self, queries, keys, values):\n",
    "        # B, L, H, E = queries.shape  # (batch_size, seq_len, n_heads, d_model // n_heads )\n",
    "        _, S, _, D = values.shape\n",
    "\n",
    "        scores = torch.einsum(\"blhe,bshe->bhls\", queries, keys) # matmul\n",
    "        scale = 1./math.sqrt(math.e) # scale\n",
    "        # no mask\n",
    "        A = torch.softmax(scale * scores, dim=-1)   # softmax\n",
    "        V = torch.einsum(\"bhls, bshd->blhd\", A, values) # matmul\n",
    "        \n",
    "        return V.contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class attentionLayer(nn.Module):\n",
    "    def __init__(self, scaled_dot_product_attention, d_model, n_heads):\n",
    "        super(attentionLayer, self).__init__()\n",
    "\n",
    "        d_values = d_model // n_heads\n",
    "\n",
    "        self.scaled_dot_product_attention = scaled_dot_product_attention\n",
    "        self.query_linear = nn.Linear(d_model, d_values * n_heads)\n",
    "        self.key_linear = nn.Linear(d_model, d_values * n_heads)\n",
    "        self.value_linear = nn.Linear(d_model, d_values * n_heads)\n",
    "        self.out_linear = nn.Linear(d_values * n_heads, d_model)\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        B, L, _ = queries.shape # (batch_size, seq_len, d_model)\n",
    "        _, S, _ = keys.shape\n",
    "        H = self.n_heads\n",
    "\n",
    "        queries = self.query_linear(queries).view(B, L, H, -1)\n",
    "        keys = self.key_linear(keys).view(B, S, H, -1)\n",
    "        values = self.value_linear(values).view(B, S, H, -1)\n",
    "\n",
    "        out = self.scaled_dot_product_attention(queries, keys, values)\n",
    "        out = out.transpose(2,1).contiguous()\n",
    "        out = out.view(B, L, -1)\n",
    "\n",
    "        return self.out_linear(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoderLayer(nn.Module):\n",
    "    def __init__(self, attention_layer, d_model, d_ff, add, norm, ff, encoder_dropout=0.05):\n",
    "        super(encoderLayer, self).__init__()\n",
    "        \n",
    "        d_ff = int(d_ff * d_model)\n",
    "        self.attention_layer = attention_layer\n",
    "\n",
    "        self.add = add\n",
    "        self.norm = norm\n",
    "        self.ff = ff\n",
    "        \n",
    "        # point wise feed forward network\n",
    "        self.feedForward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(encoder_dropout)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)    # (batch_size, seq_len, d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        new_x = self.attention_layer(x, x, x)\n",
    "        \n",
    "        if self.add:\n",
    "            if self.norm:\n",
    "                out1 = self.norm1(x + self.dropout(new_x))\n",
    "                out2 = self.norm2(out1 + self.dropout(self.feedForward(out1)))\n",
    "            else:\n",
    "                out1 = x + self.dropout(new_x)\n",
    "                out2 = out1 +  self.dropout(self.feedForward(out1))\n",
    "        else:\n",
    "            if self.norm:\n",
    "                out1 = self.norm1(self.dropout(new_x))\n",
    "                out2 = self.norm2(self.dropout(self.feedForward(out1)))\n",
    "            else:\n",
    "                out1 = self.dropout(new_x)\n",
    "                out2 = self.dropout(self.feedForward(out1))\n",
    "        \n",
    "        if self.ff:\n",
    "            return out2\n",
    "        else:\n",
    "            return out1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self, encoder_layers):\n",
    "        super(encoder, self).__init__()\n",
    "        self.encoder_layers = nn.ModuleList(encoder_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x [B, L, D]\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            x = encoder_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDAformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDAformer(nn.Module):\n",
    "    def __init__(self, seq_len, d_input, d_model=10, n_heads=2, e_layers=6, d_ff=2, pos_emb=False, value_linear=True, value_sqrt=False, add=True, norm=True, ff=True, dropout=0.05):\n",
    "        super(LDAformer, self).__init__()\n",
    "\n",
    "        # Encoding\n",
    "        self.data_embedding = dataEmbedding(d_input, d_model, posi_emb=pos_emb, value_linear=value_linear, value_sqrt=value_sqrt, input_dropout=dropout)\n",
    "        # Encoder\n",
    "        self.encoder = encoder(\n",
    "            [\n",
    "                encoderLayer(\n",
    "                    attentionLayer(scaledDotProductAttention(), d_model, n_heads),\n",
    "                    d_model,\n",
    "                    d_ff,\n",
    "                    encoder_dropout=dropout,\n",
    "                    add=add,\n",
    "                    norm=norm,\n",
    "                    ff=ff\n",
    "                ) for l in range(e_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.prediction = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(seq_len * d_model, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_embedding = self.data_embedding(x)\n",
    "        enc_out = self.encoder(x_embedding)\n",
    "        \n",
    "        return self.prediction(enc_out).squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(Ai, ij):\n",
    "    data = []\n",
    "    for item in ij:\n",
    "        feature = np.array([Ai[0][item[0]], Ai[0][item[1]]])\n",
    "        for dim in range(1, Ai.shape[0]):\n",
    "            feature = np.concatenate((feature, np.array([Ai[dim][item[0]], Ai[dim][item[1]]])))\n",
    "        data.append(feature)\n",
    "    return np.array(data)\n",
    "\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self, dataset, fold, dimension, mode='train') -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.mode = mode\n",
    "        Ai = []\n",
    "        A = np.load('data/ours/' + dataset + '/A_' + str(fold) + '.npy')\n",
    "        Ai.append(A)\n",
    "        for i in range(dimension - 1):\n",
    "            tmp = np.dot(Ai[i], A)\n",
    "            np.fill_diagonal(tmp, 0)\n",
    "            tmp = tmp / np.max(tmp)\n",
    "            Ai.append(copy.copy(tmp))\n",
    "        Ai = np.array(Ai)\n",
    "        positive_ij = np.load('data/ours/' + dataset + '/positive_ij.npy')\n",
    "        negative_ij = np.load('data/ours/' + dataset + '/negative_ij.npy')\n",
    "        positive5foldsidx = np.load('data/ours/' + dataset + '/positive5foldsidx.npy', allow_pickle=True)\n",
    "        negative5foldsidx = np.load('data/ours/' + dataset + '/negative5foldsidx.npy', allow_pickle=True)\n",
    "\n",
    "        if mode == 'test':\n",
    "            positive_test_ij = positive_ij[positive5foldsidx[fold]['test']]\n",
    "            positive_train_ij = positive_ij[positive5foldsidx[fold]['train']]\n",
    "            negative_test_ij = negative_ij[negative5foldsidx[fold]['test']]\n",
    "            negative_train_ij = negative_ij[negative5foldsidx[fold]['train']]\n",
    "            positive_test_data = torch.Tensor(get_data(Ai, positive_test_ij))\n",
    "            positive_train_data = torch.Tensor(get_data(Ai, positive_train_ij))\n",
    "            negative_test_data = torch.Tensor(get_data(Ai, negative_test_ij))\n",
    "            negative_train_data = torch.Tensor(get_data(Ai, negative_train_ij))\n",
    "            self.data = torch.cat((positive_test_data, positive_train_data, negative_test_data, negative_train_data)).transpose(1, 2)\n",
    "            self.target = torch.Tensor([1] * (len(positive_test_ij) + len(positive_train_ij)) + [0] * (len(negative_test_ij) + len(negative_train_ij)))\n",
    "\n",
    "        elif mode == 'train':\n",
    "            positive_train_ij = positive_ij[positive5foldsidx[fold]['train']]\n",
    "            positive_test_ij = positive_ij[positive5foldsidx[fold]['test']]\n",
    "            positive_train_data = torch.Tensor(get_data(Ai, positive_train_ij))\n",
    "            positive_test_data = torch.Tensor(get_data(Ai, positive_test_ij))\n",
    "            negative_train_ij = negative_ij[negative5foldsidx[fold]['test']][:(len(positive_train_ij) + len(positive_test_ij))] # Attention!\n",
    "            negative_train_data = torch.Tensor(get_data(Ai, negative_train_ij))\n",
    "            self.data = torch.cat((positive_train_data, positive_test_data, negative_train_data)).transpose(1, 2)\n",
    "            self.target = torch.Tensor([1] * (len(positive_train_ij) + len(positive_test_ij)) + [0] * len(negative_train_ij))\n",
    "\n",
    "        print('Finished reading the {} set of Dataset ({} samples found, each dim = {})'\n",
    "              .format(mode, len(self.data), self.data.shape[1:]))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.mode == 'train':\n",
    "            return self.data[index], self.target[index]\n",
    "        else:\n",
    "            return self.data[index], self.target[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_dataloader(dataset, fold, mode, dimension, batch_size, n_jobs=0):\n",
    "    ''' Generates a dataset, then is put into a dataloader. '''\n",
    "    dataset = myDataset(dataset, fold, dimension, mode=mode)  # Construct dataset\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size,\n",
    "        shuffle=(mode=='train'), drop_last=False,\n",
    "        num_workers=n_jobs, pin_memory=True)                            # Construct dataloader\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(tr_set, model, config, gpu):\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    n_epochs = config['n_epochs']\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "    loss_record = []     # for recording training loss\n",
    "    epoch = 0\n",
    "    while epoch < n_epochs:\n",
    "        model.train()                           # set model to training mode\n",
    "        for x, y in tr_set:                     # iterate through the dataloader\n",
    "            optimizer.zero_grad()               # set gradient to zero\n",
    "            x, y = x.to(gpu), y.to(gpu)   # move data to device (cpu/cuda)\n",
    "            pred = model(x)                  # forward pass (compute output)\n",
    "            bce_loss = criterion(pred, y)  # compute loss\n",
    "            bce_loss.backward()                 # compute gradient (backpropagation)\n",
    "            optimizer.step()                    # update model with optimizer\n",
    "            loss_record.append(bce_loss.detach().cpu().item())\n",
    "        epoch += 1\n",
    "\n",
    "        # if bce_loss > 1 or bce_loss < 0.05:\n",
    "        #     break\n",
    "        if bce_loss > 1:\n",
    "            break\n",
    "\n",
    "        print(epoch, bce_loss.detach().cpu().item())\n",
    "        \n",
    "    print('Finished training after {} epochs'.format(epoch))\n",
    "    return loss_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(tt_set, model, device):\n",
    "    model.eval()                                # set model to evalutation mode\n",
    "    preds = []\n",
    "    labels = []\n",
    "    for x, y in tt_set:                            # iterate through the dataloader\n",
    "        x = x.to(device)                     # move data to device (cpu/cuda)\n",
    "        with torch.no_grad():                   # disable gradient calculation\n",
    "            pred = model(x)                     # forward pass (compute output)\n",
    "            preds.append(pred.detach().cpu())   # collect prediction\n",
    "        labels.append(y.detach().cpu())\n",
    "    preds = torch.cat(preds, dim=0).numpy()     # concatenate all predictions and convert to a numpy array\n",
    "    labels = torch.cat(labels, dim=0).numpy()\n",
    "    return labels, preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config = {\n",
    "    'n_epochs': 30,\n",
    "    'batch_size': 32\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading the train set of Dataset (5394 samples found, each dim = torch.Size([1147, 6]))\n",
      "Finished reading the test set of Dataset (98880 samples found, each dim = torch.Size([1147, 6]))\n"
     ]
    }
   ],
   "source": [
    "tr_set = prep_dataloader(dataset='dataset1', fold=0, mode='train', dimension=config['dimension'], batch_size=config['batch_size'])\n",
    "tt_set = prep_dataloader(dataset='dataset1', fold=0, mode='test', dimension=config['dimension'], batch_size=config['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.4368255138397217\n",
      "2 0.04317177087068558\n",
      "3 0.2771787643432617\n",
      "4 0.1632678210735321\n",
      "5 0.36366787552833557\n",
      "6 0.09231160581111908\n",
      "7 0.04903702437877655\n",
      "8 0.1620827466249466\n",
      "9 0.15259255468845367\n",
      "10 0.5268440246582031\n",
      "11 0.12016847729682922\n",
      "12 0.4189300537109375\n",
      "13 0.022369781509041786\n",
      "14 0.11466704308986664\n",
      "15 0.011287491768598557\n",
      "16 0.17592783272266388\n",
      "17 0.11885125190019608\n",
      "18 0.20580869913101196\n",
      "19 0.018127964809536934\n",
      "20 0.2322070151567459\n",
      "21 0.07566765695810318\n",
      "22 0.049617085605859756\n",
      "23 0.040026355534791946\n",
      "24 0.1529049128293991\n",
      "25 0.012376576662063599\n",
      "26 0.008393464609980583\n",
      "27 0.03692779317498207\n",
      "28 0.11788731068372726\n",
      "29 0.13929125666618347\n",
      "30 0.4745902419090271\n",
      "Finished training after 30 epochs\n",
      "AUC = 0.9961558397800053\n",
      "AUPR = 0.9030272072528448\n"
     ]
    }
   ],
   "source": [
    "seq_len = tr_set.dataset[0][0].shape[0]\n",
    "d_input = tr_set.dataset[0][0].shape[1]\n",
    "while True:\n",
    "    model = LDAformer(\n",
    "                seq_len=seq_len, d_input=d_input, d_model=config['d_model'], \n",
    "                n_heads=config['n_heads'], d_ff=config['d_ff'], e_layers=config['e_layers'], \n",
    "                pos_emb=False, value_sqrt=False\n",
    "            ).to(gpu)\n",
    "    model_loss_record = train(tr_set, model, config, gpu)\n",
    "    if model_loss_record[-1] < 1:\n",
    "        break\n",
    "labels, preds = test(tt_set, model, gpu)\n",
    "\n",
    "AUC = roc_auc_score(labels, preds)\n",
    "print(\"AUC =\", AUC)\n",
    "precision, recall, _ = precision_recall_curve(labels, preds)\n",
    "AUPR = auc(recall, precision)\n",
    "print('AUPR =', AUPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.9961558397800053\n",
      "AUPR = 0.9030272072528448\n"
     ]
    }
   ],
   "source": [
    "# torch.save(model, 'models/LDAformer/case_study.pth')\n",
    "model = torch.load('models/LDAformer/dataset1_case_study.pth')\n",
    "labels, preds = test(tt_set, model, gpu)\n",
    "\n",
    "AUC = roc_auc_score(labels, preds)\n",
    "print(\"AUC =\", AUC)\n",
    "precision, recall, _ = precision_recall_curve(labels, preds)\n",
    "AUPR = auc(recall, precision)\n",
    "print('AUPR =', AUPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = 'dataset1'\n",
    "lnc_di = pd.read_csv('data/ours/' + ds + '/lnc_di.csv', index_col=0)\n",
    "diseases = lnc_di.columns\n",
    "lncRNAs = lnc_di.index\n",
    "\n",
    "positive_ij = np.load('data/ours/' + ds + '/positive_ij.npy')\n",
    "negative_ij = np.load('data/ours/' + ds + '/negative_ij.npy')\n",
    "positive5foldsidx = np.load('data/ours/' + ds + '/positive5foldsidx.npy', allow_pickle=True)\n",
    "negative5foldsidx = np.load('data/ours/' + ds + '/negative5foldsidx.npy', allow_pickle=True)\n",
    "positive_test_ij = positive_ij[positive5foldsidx[0]['test']]\n",
    "positive_train_ij = positive_ij[positive5foldsidx[0]['train']]\n",
    "negative_test_ij = negative_ij[negative5foldsidx[0]['test']]\n",
    "negative_train_ij = negative_ij[negative5foldsidx[0]['train']]\n",
    "\n",
    "ij = np.concatenate((positive_test_ij, positive_train_ij, negative_test_ij, negative_train_ij))\n",
    "i = ij[:, 0].T\n",
    "j = ij[:, 1].T\n",
    "labels = labels.astype(int)\n",
    "prediction_results = pd.DataFrame({\n",
    "        'lncRNA': np.array([lncRNAs[lncRNA] for lncRNA in i]),\n",
    "        'disease': np.array([diseases[disease - len(lncRNAs)] for disease in j]),\n",
    "        'pred': preds,\n",
    "        'label': labels\n",
    "    })\n",
    "\n",
    "evidence = pd.read_csv('data/ours/dataset2/union/di_lnc_union.csv', index_col='Unnamed: 0')\n",
    "evidence_diseases = evidence.index\n",
    "evidence_lncRNAs = evidence.columns\n",
    "\n",
    "new_results = pd.DataFrame(columns=['lncRNA', 'disease', 'pred', 'label', 'evidence'])\n",
    "for idx, row in prediction_results.iterrows():\n",
    "    lncRNA = row['lncRNA']\n",
    "    disease = row['disease']\n",
    "    evd = 0\n",
    "    if (lncRNA in evidence_lncRNAs) and (disease in evidence_diseases) and (evidence.loc[disease, lncRNA] == 1):\n",
    "        evd = 1\n",
    "    new_results = new_results.append({\n",
    "        'lncRNA': lncRNA,\n",
    "        'disease': disease,\n",
    "        'pred': row['pred'],\n",
    "        'label': row['label'],\n",
    "        'evidence': evd\n",
    "    }, ignore_index=True)\n",
    "new_results.sort_values(by='pred', ascending=False).to_csv('files/case study/dataset1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOID:0050865\n",
      "DOID:0050866\n",
      "DOID:10283\n",
      "DOID:10534\n",
      "DOID:11054\n",
      "DOID:1324\n",
      "DOID:1380\n",
      "DOID:1612\n",
      "DOID:162\n",
      "DOID:1749\n",
      "DOID:1781\n",
      "DOID:1793\n",
      "DOID:1909\n",
      "DOID:2152\n",
      "DOID:219\n",
      "DOID:2394\n",
      "DOID:2876\n",
      "DOID:3068\n",
      "DOID:3070\n",
      "DOID:3121\n",
      "DOID:3347\n",
      "DOID:3498\n",
      "DOID:3571\n",
      "DOID:3748\n",
      "DOID:3907\n",
      "DOID:3908\n",
      "DOID:3910\n",
      "DOID:3969\n",
      "DOID:4362\n",
      "DOID:4450\n",
      "DOID:4467\n",
      "DOID:47\n",
      "DOID:5041\n",
      "DOID:5520\n",
      "DOID:684\n",
      "DOID:768\n",
      "DOID:769\n",
      "DOID:9119\n",
      "DOID:9256\n",
      "DOID:9261\n",
      "DOID:9538\n"
     ]
    }
   ],
   "source": [
    "for di, case in list(new_results[new_results['evidence'] == 1].groupby(['disease'])):\n",
    "    if len(case.values) > 10:\n",
    "        print(di)\n",
    "        new_results[(new_results['disease'] == di) & (new_results['label'] == 0)].sort_values(by='pred', ascending=False).to_csv('files/case study/' + di + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lncRNA</th>\n",
       "      <th>disease</th>\n",
       "      <th>pred</th>\n",
       "      <th>label</th>\n",
       "      <th>evidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1369</th>\n",
       "      <td>H19</td>\n",
       "      <td>DOID:162</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1652</th>\n",
       "      <td>GAS5</td>\n",
       "      <td>DOID:4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2104</th>\n",
       "      <td>MALAT1</td>\n",
       "      <td>DOID:4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2101</th>\n",
       "      <td>H19</td>\n",
       "      <td>DOID:1612</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>MEG3</td>\n",
       "      <td>DOID:14566</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72212</th>\n",
       "      <td>LINC00261</td>\n",
       "      <td>DOID:77</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18926</th>\n",
       "      <td>LINC-ROR</td>\n",
       "      <td>DOID:77</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64420</th>\n",
       "      <td>LINC00467</td>\n",
       "      <td>DOID:77</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51284</th>\n",
       "      <td>FENDRR</td>\n",
       "      <td>DOID:77</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76626</th>\n",
       "      <td>LINC00472</td>\n",
       "      <td>DOID:77</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98880 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          lncRNA     disease      pred  label  evidence\n",
       "1369         H19    DOID:162  1.000000      1         1\n",
       "1652        GAS5      DOID:4  1.000000      1         0\n",
       "2104      MALAT1      DOID:4  1.000000      1         0\n",
       "2101         H19   DOID:1612  1.000000      1         1\n",
       "517         MEG3  DOID:14566  1.000000      1         0\n",
       "...          ...         ...       ...    ...       ...\n",
       "72212  LINC00261     DOID:77  0.000003      0         0\n",
       "18926   LINC-ROR     DOID:77  0.000003      0         0\n",
       "64420  LINC00467     DOID:77  0.000003      0         0\n",
       "51284     FENDRR     DOID:77  0.000002      0         0\n",
       "76626  LINC00472     DOID:77  0.000002      0         0\n",
       "\n",
       "[98880 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1_results = pd.read_csv('files/case study/dataset1/dataset1.csv', index_col=0)\n",
    "dataset1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1860"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset1_results[(dataset1_results['evidence'] == 1)].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1258"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset1_results[((dataset1_results['evidence'] == 1) & (dataset1_results['label'] == 0))].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "280"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset1_results[((dataset1_results['evidence'] == 1) & (dataset1_results['label'] == 0) & (dataset1_results['pred'] > 0.5))].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lncRNA</th>\n",
       "      <th>disease</th>\n",
       "      <th>pred</th>\n",
       "      <th>label</th>\n",
       "      <th>evidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86633</th>\n",
       "      <td>H19</td>\n",
       "      <td>DOID:9119</td>\n",
       "      <td>0.999958</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39858</th>\n",
       "      <td>MALAT1</td>\n",
       "      <td>DOID:8649</td>\n",
       "      <td>0.999939</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91785</th>\n",
       "      <td>EWSAT1</td>\n",
       "      <td>DOID:3347</td>\n",
       "      <td>0.999908</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82481</th>\n",
       "      <td>CDKN2B-AS1</td>\n",
       "      <td>DOID:4362</td>\n",
       "      <td>0.999887</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43584</th>\n",
       "      <td>H19</td>\n",
       "      <td>DOID:114</td>\n",
       "      <td>0.999838</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46852</th>\n",
       "      <td>GAS5</td>\n",
       "      <td>DOID:0050866</td>\n",
       "      <td>0.507397</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69872</th>\n",
       "      <td>MIR100HG</td>\n",
       "      <td>DOID:9256</td>\n",
       "      <td>0.505816</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98688</th>\n",
       "      <td>NEAT1</td>\n",
       "      <td>DOID:3068</td>\n",
       "      <td>0.503372</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67615</th>\n",
       "      <td>PVT1</td>\n",
       "      <td>DOID:0050866</td>\n",
       "      <td>0.502477</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24827</th>\n",
       "      <td>CCAT2</td>\n",
       "      <td>DOID:3498</td>\n",
       "      <td>0.501097</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>280 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           lncRNA       disease      pred  label  evidence\n",
       "86633         H19     DOID:9119  0.999958      0         1\n",
       "39858      MALAT1     DOID:8649  0.999939      0         1\n",
       "91785      EWSAT1     DOID:3347  0.999908      0         1\n",
       "82481  CDKN2B-AS1     DOID:4362  0.999887      0         1\n",
       "43584         H19      DOID:114  0.999838      0         1\n",
       "...           ...           ...       ...    ...       ...\n",
       "46852        GAS5  DOID:0050866  0.507397      0         1\n",
       "69872    MIR100HG     DOID:9256  0.505816      0         1\n",
       "98688       NEAT1     DOID:3068  0.503372      0         1\n",
       "67615        PVT1  DOID:0050866  0.502477      0         1\n",
       "24827       CCAT2     DOID:3498  0.501097      0         1\n",
       "\n",
       "[280 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1_results[((dataset1_results['evidence'] == 1) & (dataset1_results['label'] == 0) & (dataset1_results['pred'] > 0.5))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config = {\n",
    "    'n_epochs': 10,\n",
    "    'batch_size': 32\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading the train set of Dataset (7666 samples found, each dim = torch.Size([1276, 6]))\n",
      "Finished reading the test set of Dataset (210140 samples found, each dim = torch.Size([1276, 6]))\n"
     ]
    }
   ],
   "source": [
    "tr_set = prep_dataloader(dataset='dataset2', fold=0, mode='train', dimension=config['dimension'], batch_size=config['batch_size'])\n",
    "tt_set = prep_dataloader(dataset='dataset2', fold=0, mode='test', dimension=config['dimension'], batch_size=config['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.18491390347480774\n",
      "2 0.11688053607940674\n",
      "3 0.452679306268692\n",
      "4 0.3152638077735901\n",
      "5 0.14764411747455597\n",
      "6 0.30353203415870667\n",
      "7 0.224164679646492\n",
      "8 0.5397985577583313\n",
      "9 0.37905147671699524\n",
      "10 0.28358888626098633\n",
      "Finished training after 10 epochs\n",
      "AUC = 0.9574395486089452\n",
      "AUPR = 0.5151922682331465\n"
     ]
    }
   ],
   "source": [
    "seq_len = tr_set.dataset[0][0].shape[0]\n",
    "d_input = tr_set.dataset[0][0].shape[1]\n",
    "while True:\n",
    "    model = LDAformer(\n",
    "                seq_len=seq_len, d_input=d_input, d_model=config['d_model'], \n",
    "                n_heads=config['n_heads'], d_ff=config['d_ff'], e_layers=config['e_layers'], \n",
    "                pos_emb=False, value_sqrt=False\n",
    "            ).to(gpu)\n",
    "    model_loss_record = train(tr_set, model, config, gpu)\n",
    "    if model_loss_record[-1] < 1:\n",
    "        break\n",
    "labels, preds = test(tt_set, model, gpu)\n",
    "\n",
    "AUC = roc_auc_score(labels, preds)\n",
    "print(\"AUC =\", AUC)\n",
    "precision, recall, _ = precision_recall_curve(labels, preds)\n",
    "AUPR = auc(recall, precision)\n",
    "print('AUPR =', AUPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.9574395486089452\n",
      "AUPR = 0.5151922682331465\n"
     ]
    }
   ],
   "source": [
    "# torch.save(model, 'models/LDAformer/dataset2_case_study.pth')\n",
    "model = torch.load('models/LDAformer/dataset2_case_study.pth')\n",
    "\n",
    "labels, preds = test(tt_set, model, gpu)\n",
    "\n",
    "AUC = roc_auc_score(labels, preds)\n",
    "print(\"AUC =\", AUC)\n",
    "precision, recall, _ = precision_recall_curve(labels, preds)\n",
    "AUPR = auc(recall, precision)\n",
    "print('AUPR =', AUPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = 'dataset2'\n",
    "lnc_di = pd.read_csv('data/ours/' + ds + '/intersection/di_lnc_intersection.csv', index_col=0)\n",
    "diseases = lnc_di.index\n",
    "lncRNAs = lnc_di.columns\n",
    "\n",
    "positive_ij = np.load('data/ours/' + ds + '/positive_ij.npy')\n",
    "negative_ij = np.load('data/ours/' + ds + '/negative_ij.npy')\n",
    "positive5foldsidx = np.load('data/ours/' + ds + '/positive5foldsidx.npy', allow_pickle=True)\n",
    "negative5foldsidx = np.load('data/ours/' + ds + '/negative5foldsidx.npy', allow_pickle=True)\n",
    "positive_test_ij = positive_ij[positive5foldsidx[0]['test']]\n",
    "positive_train_ij = positive_ij[positive5foldsidx[0]['train']]\n",
    "negative_test_ij = negative_ij[negative5foldsidx[0]['test']]\n",
    "negative_train_ij = negative_ij[negative5foldsidx[0]['train']]\n",
    "\n",
    "ij = np.concatenate((positive_test_ij, positive_train_ij, negative_test_ij, negative_train_ij))\n",
    "i = ij[:, 0].T\n",
    "j = ij[:, 1].T\n",
    "labels = labels.astype(int)\n",
    "prediction_results = pd.DataFrame({\n",
    "        'lncRNA': np.array([lncRNAs[lncRNA] for lncRNA in i]),\n",
    "        'disease': np.array([diseases[disease - len(lncRNAs)] for disease in j]),\n",
    "        'pred': preds,\n",
    "        'label': labels\n",
    "    })\n",
    "# prediction_results.to_csv('files/case study/dataset2/dataset2.csv')\n",
    "prediction_results[prediction_results['label'] == 0].sort_values(by='pred', ascending=False).to_csv('files/case study/dataset2/dataset2_label0.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
