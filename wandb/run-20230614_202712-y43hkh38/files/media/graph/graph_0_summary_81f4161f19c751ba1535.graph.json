{"format": "torch", "nodes": [{"name": "ganlayer", "id": 2423255153584, "class_name": "GANLayer(\n  (gat_layers): ModuleList(\n    (0): GraphTransformerLayer(in_channels=64, out_channels=64, heads=1, residual=True)\n  )\n  (embedding_lap_pos_enc): Linear(in_features=64, out_features=64, bias=True)\n  (embedding_h): Linear(in_features=1147, out_features=64, bias=True)\n)", "parameters": [["gat_layers.0.attention.Q.weight", [64, 64]], ["gat_layers.0.attention.K.weight", [64, 64]], ["gat_layers.0.attention.V.weight", [64, 64]], ["gat_layers.0.O.weight", [64, 64]], ["gat_layers.0.O.bias", [64]], ["gat_layers.0.batch_norm1.weight", [64]], ["gat_layers.0.batch_norm1.bias", [64]], ["gat_layers.0.FFN_layer1.weight", [128, 64]], ["gat_layers.0.FFN_layer1.bias", [128]], ["gat_layers.0.FFN_layer2.weight", [64, 128]], ["gat_layers.0.FFN_layer2.bias", [64]], ["gat_layers.0.batch_norm2.weight", [64]], ["gat_layers.0.batch_norm2.bias", [64]], ["embedding_lap_pos_enc.weight", [64, 64]], ["embedding_lap_pos_enc.bias", [64]], ["embedding_h.weight", [64, 1147]], ["embedding_h.bias", [64]]], "output_shape": [[1147, 64]], "num_parameters": [4096, 4096, 4096, 4096, 64, 64, 64, 8192, 128, 8192, 64, 64, 64, 4096, 64, 73408, 64]}, {"name": "mlplayer", "id": 2423255153440, "class_name": "MLPLayer(\n  (mlp_layer1): Linear(in_features=128, out_features=512, bias=True)\n  (mlp_layer2): Linear(in_features=512, out_features=512, bias=True)\n  (mlp_layer3): Linear(in_features=512, out_features=1, bias=True)\n)", "parameters": [["mlp_layer1.weight", [512, 128]], ["mlp_layer1.bias", [512]], ["mlp_layer2.weight", [512, 512]], ["mlp_layer2.bias", [512]], ["mlp_layer3.weight", [1, 512]], ["mlp_layer3.bias", [1]]], "output_shape": [[4314]], "num_parameters": [65536, 512, 262144, 512, 512, 1]}], "edges": []}